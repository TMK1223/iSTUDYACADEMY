# 深層学習ー後編

## section1:再帰型ニューラルネットワーク

- 時系列データに対応可能
- 三つの重みを用いる、入力から現在の中間層、中間層から出力、次の循環層の重み
- 初期の状態と過去の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造が必要になる

- BPTT：RNNにおいてのパラメタ調整方法の一種

$$
z_1=sigomid(S_ow+xw_{in}+b)\\
y_1=sigmoid(z_1w_{out}+c)
$$

- RNNの課題：時系列を遡れば遡るほど、勾配が消失していく

## section2:LTSM

- LSTM：RNNの中間層のユニットをLSTM blockと呼ばれるメモリと3つのゲートを持つブロックに置き換える
- CEC：勾配消失および勾配爆発の解決方法として、勾配が1であれば解決できる
- 課題として入力データについて時間依存度に関係なく重みが一律である
- 入力ゲート、出力ゲート
  - それぞれのゲートへの入力値の重みを、重み行列W,Uで可変可能とする。
- 忘却ゲート：古いデータを取り除くゲート
- 覗き穴結合：CSC自身の値に、重み行列を介して伝播可能にした構造

## section3:GRU

- LTSMでは、パラメータが多く、計算負荷が高くなる問題があった
- GRUはパラメータを大幅に削減し、尚且つLTSMと精度は同等かそれ以上が望める

## section4:双方向RNN

- 過去の情報だけでなく、未来の情報を加味することで精度を向上させるモデル
- イメージとしては順伝播と逆伝播を合わせる

## section5:Seq2Seq

- Encoder RNN:ユーザがインプットしてきたテキストデータを単語等のトークンに区切って渡す構造
  - Taking:文書を単語等のトークン毎に分割してトークン毎のIDを付与する
  - Embedding：IDからそのトークンを表す分散表現ベクトルに変換
  - EncoderRNN：ベクトルを順番にRNNに入力していく
- 処理手順
  - vec1をRNNに入力し、hidden stateを出力
  - Hidden stateと次の入力vec2をまたRNNに入力以上を繰り返す
  - 最後のvecを入れた時のhidden state をfinal stateとして取っておく
  - Final stateがthought vectorと呼ばれ、入力した文の意味を表すベクトルとなる
- DecoderRNN：システムがアウトプットデータを単語等のトークン毎に生成する構造
  - EncoderRNNのfinalstateから、各tokenの生成確率を出力していく
  - Final state をDecoderRNNのinitial stateととして設定してEmbeddingを入力
  - Sampling：生成確率に基づいてtokenをランダムに選びます
  - Embedding：選ばれたtokenをEmbeddingしてDecoderRNNへの次の入力とする
  - Detokenize：上記を繰り返し、tokenを文字列に直す
- HRED
  - Seq2Seqの問題点として一問一答しかできない
  - 過去N-1この発話から次の発話を生成する
  - HRED＝Seq2seq+ContextRNN
  - ContextRNN：Encoderのまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造
  - HREDの課題
    - 確率的な多様性が字面にしかなく、会話の流れがない
    - 短く情報量に乏しい答えになることが多い
- VHRED
  - HREDにVAEの潜在変数の概念を追加したもの
- VAE
  - オートエンコーダ：教師なし学習の一種、次元削減が行える
  - VAE：潜在変数zに確率的分布を仮定したもの

## section6:Word2Vec

- 学習データからボキャブラリを作成
- One-hotベクトル：適合する単語に辞書内でベクトルが付与される
- メリット：大規模データの分散表現の学習が、現実的な計算速度とメモリ量で実現可能にした



## section7:Attention Mechanism

- 入力と出力のどの単語が関係しているのかの関連度を学習する仕組み



## section8:TensorFlow

- Googleが開発したオープンソース



## section9:強化学習

- 長期的に報酬を最大化できるように環境内で行動を選択できるエージェントを作ることを目標とした手法
- 行動の結果の利益をもとに行動を決定する原理を改善する仕組み
- マーケティングで顧客毎へのキャンペーンの通知をするかどうか等
- 基本的には不完全な知識を元に行動しながら、データを収集する
- 方策関数から行動を決定し、結果の状態を取得して行動価値関数により報酬を得る
- 強化学習は教師あり、教師なし学習とは目標が違い、優れた方策を見つけ出すのが目標になる
- 行動価値関数：価値を表す関数として状態価値関数と行動価値関数がある
  - 状態価値関数：ある状態の価値に注目する
  - 行動価値関数：状態と価値を組み合わせた価値に注目する
- 方策関数：方策ベースの強化学習手法において、ある状態でどのような行動を採るのかの確立を与える関数
- 方策勾配法：方策をモデル化して最適化する手法
  - 平均報酬：行動を取った時に生まれる報酬の全部の平均から
  - 割引報酬和：カッコになればなるほどその報酬の加算する割合を減す＝減衰



## 深層学習後半ー演習

https://github.com/TMK1223/iSTUDYACADEMY/blob/master/深層学習後編.ipynb



