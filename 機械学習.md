# 機械学習

## 線形回帰

ある入力（数値）から出力（連続値）を予測する問題

⇨回帰問題を解くための学習モデル

#### 回帰で扱うデータ

- 入力はm次元のベクトル
- 入力ベクトルの各要素は説明変数または特徴量と呼ぶ
- 出力はスカラー値（目的変数）と呼ぶ



#### 線形回帰モデル

- 教師あり学習＝正解付きデータから学習
- 入力とm次元パラメータの線型結合を出力するモデル

$$
w = (w_1+w_2+w_3+・・・w_m)^T\in R^m
$$



#### 線型結合

- 説明変数とパラメータ変数の内積に切片追加したもの
- 入力のベクトルが多次元でも出力は一次元（スカラ）となる

$$
\hat{y}=w^Tx+b-\sum_{j=1}^{m}w_jx_j+b
$$

#### 

#### 線形回帰モデルのパラメータ

- 特徴量が予測値に対してどのように影響を与えるかを決定する重みの集合
- 正の（負の）重みをつける場合、その特徴量の値を増加させると、予測の値が増加（減少）
- 線形結合は直線、面を表している

*未知パラメーターは最小二乗法により推定*



## データ分割・学習

- 学習用データでモデルを学習し、検証用データでモデルを検証する

#### 機械学習モデルの評価と選択

- 正解付きデータを学習用と検証用に分けそれぞれ作成したモデルに学習その後検証して適切なモデルを判断

#### パラメータの推定

##### 平均二乗誤差（Mean Squared Error）

- データとモデル出力の二乗誤差
- 小さいほど直線とデータの距離が近い＝精度が高い

$$
MSE_{train}=\frac{1}{n_{train}}\sum_{t=1}^{n_{train}}(\hat{y}_t^{train}-y_t^{train})^2
$$



##### 最小二乗法

- 学習データの平均二乗誤差を最小とするパラメータを探索
- 学習データの平均二乗誤差の最小化はその勾配が0になる点を求める

MSEを最小にするようなW（m次元）
$$
\hat{w}=arg\min_{w\in R~m}MSE_{train}
$$
MSEをWに関して微分したものが0となるWの点を求める
$$
\frac{\partial}{\partial w}MSE_{train}=0
$$
パラメータの推定値をもとに予測値は以下になる
$$
\hat{y}=X(X^{(train)T}X^{train})^{-1}X^{(train)T}y
$$

## 非線形回帰モデル

#### 基底展開法

- 回帰係数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を使用
- 未知パラメータは線形と同じく最小二乗法や最尤法により推定

$$
y_i=w_0+\sum_{i=1}^{m}w_j\phi_j(x_i)+\epsilon_i
$$

説明変数
$$
x = (x_1+x_2+x_3+・・・x_m)^T\in R^n
$$
非線形関数ベクトル
$$
\phi_j(x)=(\phi_j(x_1),\phi_j(x_1),・・・,\phi_j(x_n))^T
$$
非線形関数の計画行列
$$
\Phi^{(train)}=(\phi_j(x),\phi_j(x),・・・,\phi_j(x))^T
$$
最尤法による予測値
$$
\hat{y}=\Phi(\Phi^{(train)T}\Phi^{(train)})^{-1}\Phi^{(train)T}y
$$

## 正則化法

##### 過学習と未学習

- 学習データに対して、十分小さな誤差が得られない場合＝未学習
- 小さい誤差は得られたが、テスト集合誤差との差が大きい場合＝過学習

*⇨過学習は正則化法で回避*

##### 汎化性能

- 学習に使用した入力だけでなく、これまで見たことのない新たな入力に対する予測性能
- 汎化性能＝テスト誤差が小さいものが良い性能のモデル
- 新しい入力に対する誤差の期待値で定義される

$$
MSE_{train}=モデルの学習に使用\\
MSE_{test}=モデルの性能の指標
$$

##### 背景

- 基底関数の数やバンド幅によりモデルの複雑さが変化
- 適切な複雑さを持つモデルを使用しないと過学習や未学習が発生
- モデルの複雑さに伴って、その値が大きくなるペナルティ項を貸した関数の最小化を考える
- 正則化パラメータまたは平滑化パラメータがモデルの曲線のなめらかさを調整するとともに推定量の安定に寄与する

$$
S_\gamma=(y-\Phi w)^T(y-\Phi w)+\gamma R(w)
$$

##### ペナルティ項

##### ノルムについて

$$
L^2ノルム：\sqrt{x^2_1+x^2_2+・・・x^2_n}\\
L^1ノルム：|x_1|+|x_2|+・・・+|x_n|
$$



- 正則化法：過学習を回避する方法　モデルの複雑さに伴いペナルティを課す

  - Ridge推定量(L2ノルム)：MSEの値をパラメータ空間の原点に近い所(0)に近づける（縮小推定）

  - Lasso推定量(L1ノルム)：MSEの値をパラメータの一部を0になるように近づける→次元削減（スパース推定）

  

## モデルについて

-  正則化パラメータ（基底位置・バンド幅）はクロスバリデーションで選択する

1. ホールドアウト法

- 手元のデータを学習用とテスト用に分割する方法
  - 有限のデータを分けるため学習精度と性能評価がトレードオフ

2. クロスバリデーション

- 手元の各クラスのデータをそれぞれm個のグループに分割しmー1個のグループのデータを使って学習して残った一個をテスト用のデータにする方法
- m回繰り返し、それらの平均二乗誤差の平均を性能予測値とする



## ロジスティック回帰

#### 分類問題（クラス分類）

1. ロジスティック回帰モデル
   - 分類問題を解くための機械学習モデル
   - 入力とパラメータの線型結合をシグモイド関数に入力する
   - ある入力に対する出力が1になる確率を表現

未知パラメータ
$$
\hat{y}=w^Tx+b-\sum_{j=1}^{m}w_jx_j+b
$$

2. シグモイド関数
   - 入力は実数
   - 出力は0か1
   - パラメータが変わるとシグモイド関数の値が変わる

$$
\sigma(x)=\frac{1}{1+\exp(-ax)}
$$

シグモイド関数の微分
$$
\frac{\partial\sigma(x)}{\partial x}=a\partial(x)(1-\partial(x))
$$
ロジスティック回帰モデル：Y=1になる確率とシグモイド関数を対応させる

左辺：未知のデータが与えられた際にY=1になる確率

右辺：データのパラメータに対する線型結合
$$
P(Y=1|x)=\sigma(w_0+w_1x_1+・・・+w_mx_m)
$$

## 最尤推定

- 尤度とはあるデータを得たときに、分布のパラメータが特定の値であることがどれほどあり得そうか（尤もらいしいか）を表現したもの
- 確率変数が独立した場合は同時分布がそれぞれの分布の積である。



#### 尤度関数

- 確率変数Y（実数値として0か1を取る）はベルヌーイ分布に従う
- 確率変数Yはベルヌーイ試行に従う

$$
P(Y=t|x)=p^t(1-p)^{1-t}
$$

#### 同時確率（パラメータの関数⇨尤度関数）

- 学習データセットが同時に得られる確率を計算
- 学習データを発生させる尤もらしい確率分布を求める

$$
P(Y=y_1|x_1)P(Y=y_2|x_2)・・・P(Y=y_n|x_n)=\prod_{i=1}^{n}p^{y_i}_i(1-p_i)^{1-y_i}
$$

- 尤度関数を最大にするパラメータを推定パラメータとする

#### 対数尤度関数の最大化

- 対数尤度関数が最大になる点と尤度関数が最大になる点は同じ

$$
E(w_1,w_2,・・・,w_m)=-\log L(w_1,w_2,・・・,w_m)=-\sum(t_n\log p_i+(1-t_n)\log(1-p_i))
$$



## 勾配降下法

- 線形回帰モデル⇨MSEのパラメータに関する微分が0になる値を解析に求めることが可能
- ロジスティック回帰⇨対数尤度関数をパラメータで微分して0になる値を求めるのは困難なため
- 反復学習によりパラメータを逐次的に更新するアプローチの一つ
- ηは学習率と呼ばれるハイパーパラメータでモデルのパラメータの収束しやすさを調整

$$
w^{k+1}=w^k-\eta\frac{\partial E(w,b)}{\partial w}\\
b^{k+1}=b^k-\eta\frac{\partial E(w,b)}{\partial b}
$$

- パラメータが更新されなくなった場合、勾配が0になり反復学習の範囲では最適な解が求められた
- 勾配降下法では、パラメータを更新するのにN個全てデータに対する和を求める必要がある
  - nが巨大になった場合メモリ足りなくなる恐れあり
  - 確率的勾配降下法で回避可能

#### 確率的勾配降下法(SGD)

- データを一つずつランダムに選んでパラメータを更新
- n回で勾配が0になることは稀、繰り返し学習する必要がある
- データ全体に対する反復回数をエポックと呼ぶ。
- 各エポックごとのデータをまたシャッフルして学習することで学習に偏りが生じにくくなる

## モデルの評価

1. 正解率
   1. 正解したデータと予測した全データの割合
   2. TP+TN/TP+FN+FP+TN
2. 適合率
   1. 見流しを許容して、誤判定を許容しない場合
   2. TP/TP+FP
3. 再現率
   1. 抜け漏れを少なくしたい場合
   2. TP/TP+FN
4. F値
   1. 二つのトレードオフが起こっている場合



## ハンズオン

https://github.com/TMK1223/iSTUDYACADEMY/blob/master/機械学習.ipynb

## 主成分分析

- 多変量データの持つ構造を少数この指標にまとめる
- 学習データの分散が最大になる方向への線形変換を求める手法

学習データ
$$
x=(x_1+x_2+・・・+x_m)^T\in R^m
$$
平均ベクトル
$$
\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i
$$
平均ベクトルを引いたデータ行列
$$
\bar{X}=(x_1-\bar{x}+・・・＋x_n-\bar{x})^T
$$
分散共分散行列
$$
\sum=Var(\bar{X})=\frac{1}{n}\bar{X}^T\bar{X}
$$
n個のデータを係数ベクトルを用いて線形変換
$$
S_j=(s_{1j},・・・,s_{nj})^T=\bar{X}a_j
$$

- 係数ベクトルが変われば線形変換後の値が変わる
- 情報の量を分散の大きさで捉えて、変換後の分散が最大となる射影軸を探索
- 上限の制約付き最適化問題はラグランジュ関数を最大にする系列ベクトルを見つける

$$
E(a_j)=a_j^TVar(\bar{X})a_j-\lambda(a_j^Ta_j-1)\\
\frac{\partial E(a_j)}{\partial a_j}=2Var(\bar{X})a_j-2\lambda a_j=0
$$

- 分散の値は固有値と同じ
- 主成分：K番目の固有値に対する固有ベクトルで変換された特徴量を第k主成分と呼ぶ
- 寄与率：第k主成分の分散の全分散に対する割合を第k成分の寄与率という

## K近傍法/K平均法

#### K近傍法

- 教師あり学習

- 近傍K個のデータから識別、K個のクラスラベルの中で最も多いラベルを割り当てる
- Kを大きくすると決定境界は滑らかになる

#### K平均法

- 教師なし学習
- 与えられたデータをK個のクラスタに分類する

アルゴリズム

1. 各クラスタの初期値を設定
2. 各データ点に対して、各クラスタ中心との距離を計算して、最も距離が近いクラスタを割り当てる
3. 各クラスタの平均ベクトルを計算
4. 収束するまで繰り返す



