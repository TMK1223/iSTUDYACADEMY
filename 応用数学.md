

# 応用数学

## 線形代数

固有値・固有ベクトルについての求め方の確認

#### ベクトルの特徴

1. [大きさ]と[向き]を持っている

2. スカラーのセットで表示される

#### 行列

- ベクトル変換に用いる

#### 固有値と固有ベクトル

ある行列Aに対して，以下のような式が成り立つような，特殊な ベクトル𝑥Ԧと，右辺の係数λがある。 

A𝑥Ԧ = 𝜆𝑥Ԧ 行列Aとその特殊なベクトル𝑥Ԧの積は，ただのスカラーの数λと 

その特殊なベクトル𝑥Ԧとの積と同じ値になる! 

この特殊なベクトル𝑥Ԧとその係数λを，行列Aに対する，固有ベ クトル，固有値という。

 

#### 固有値分解

このλを**固有値**(eigenvalue)、v を固有ベクトル(eigenvector)という。 λを大きさの順で並べ替え、これを対角にする対角ベクトルをつくる。 また対応するv を同順で列にして行列 V をつくる。 A = VΛV-1 これをA の**固有値分解**という。



#### 特異値分解

正方行列以外でも固有値分解のようなことをする方法
$$
M\vec{\nu} =\sigma\vec{\mu}\\
M^T\vec{\mu} =\sigma\vec{\nu}
$$
上記のような単位ベクトルがあれば特異値分解できる
$$
M=USV^{-1}
$$


#### 行列式

ある一つの正方行列に、ある一つの数値が対応する





## 確率

#### 頻度確率

- 発生する頻度

#### ベイズ確率（主観確率）

- 信念の度合い

#### 独立な事象の同時確率

- お互いの発生には因果関係のない事象X=xと事象Y=yが同時に発生する確率

$$
𝑃(𝑋=𝑥,𝑌=𝑦) = 𝑃(𝑋=𝑥)𝑃(𝑌=𝑦) = 𝑃(𝑌 = 𝑦, 𝑋 = 𝑥)
$$

#### 条件付き確率

- ある事象X=xが与えられた下で，Y=yとなる確率

$$
𝑃(𝑌=𝑦｜𝑋=𝑥)= \frac{𝑃 (𝑌 = 𝑦, 𝑋 = 𝑥)}{𝑃(𝑋=𝑥)}
$$



#### 確率変数

- 事象と結びつけられた数値
- 事象そのものを指すと解釈する場合もある

#### 確率分布

- 事象の発生する確率の分布
- 離散値であれば表に出せる

#### 期待値

- その分布における平均値、または「あり得そうな数値」

#### 期待値E（f）

$$
=\sum{P}(X=x_k)f（X=x_k)
$$

#### 分数

- データの散らばり具合
- データの各々の値が期待値からどれくらいずれているのか平均したもの

分散Var(f)
$$
=E((f_{X=x}-E_{(f)})^2)\\
=E(f^2_{X=x})-(E_{(f)})^2
$$


#### 共分散

- 二つのデータ系列の傾向の違い
- 正の値は似た傾向、負の値は逆の傾向
- ゼロをとれば関係性に乏しい

共分散Cov(f,g)
$$
=E((f_{X=x}-E_{(f)})(g_{X=x}-E_{(f)}))\\
=E(fg)-(E_{(f)})(E_{(g)})
$$


#### ベルヌーイ分布

- コイントスのイメージ
- 二つの事象の確率が等しくなくても扱える

$$
P(X|\mu)=\mu^x(1-\mu)^{1-x}
$$

#### マルチヌーイ分布（カテゴリアル分布）

- サイコロ転がすイメージ
- 各面のでる割合が等しくなくても扱える

#### 二項分布

- ベルヌーイ分布の多試行版

$$
P(x|\lambda,n)=\frac{n!}{x!(n-x)!}\lambda^x(1-\lambda)^{n-x}
$$

#### ガウス分布

- 釣鐘型の連続分布

$$
N(x;\mu,\sigma^2)=\sqrt{\frac{1}{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
$$

## 情報理論

#### 自己情報量

- 対数の底が2の時、単位はビット
- 対数の底がeの時、単位はnat

$$
I(x)=-\log(P(x))=\log(W(x))
$$

#### シャノンエントロピー

- 自己情報量の期待値
- 自己情報量の加重平均で求めている

$$
H(x)=-\sum(P(x)\log(P(x))
$$

#### KL ダイバージェンス

- 同じ事象、確率変数における異なる確率分布P、Qの違いを表す

$$
D_{KL}(P||Q)=\mathbb{E}_{x~P}[\log\frac{P(x)}{Q(x)}]=\mathbb{E}_{x~P}[\log(P(x))-\log(Q(x))]
$$

#### 交差エントロピー

- KLダイバージェンスの一部分を取り出したもの
- Qについての自己情報量をPの分布で平均している

$$
H(P,Q)=H(P)+D_{KL}(P||Q)
H(P,Q)=-\mathbb{E}_{x~P}\log(Q(x))
$$



