# 深層学習　前半1

## ニューラルネットワーク全体像

- 入力層に与えらた値それぞれに重みがかけられて最後にバイアスが足されて、中間層に渡される。
- 重み、バイアスに関してはそれぞれの入力層、中間層に対して与えられている
- そもそもの深層学習自体の目的は誤差を最小化するパラメータを発見すること
  - 重みとバイアスが上記パラメータに含まれる

- NNできること
  - 回帰：予測すること
    - 結果予想や予想ランキング
  - 分類：判別やクラス分類
    - 写真の判別
    - 文字認識（OCR）
- 深層学習の実用例
  - 自動売買
  - チャットボット
  - 翻訳etc

## section1:入力層から中間層

- 入力層の値それぞれに対して重みをかけてバイアスを足す、結果として出力が次の層：中間層に渡される。

![UNADJUSTEDNONRAW_thumb_bda](/Users/koiketomoki/Pictures/写真ライブラリ.photoslibrary/resources/proxies/derivatives/0b/00/bda/UNADJUSTEDNONRAW_thumb_bda.jpg)

![深層学習前編](/Users/koiketomoki/Pictures/写真ライブラリ.photoslibrary/Masters/2019/06/24/20190624-102119/深層学習前編.png)



## section2:活性化関数

- NNにおいて次の層への出力の大きさを決める非線形の関数
- 次の層に渡すに当たって値に強弱をつける



線形と非線形の違い

1. $$
   f(x+y)=f(x)+f(y)
   $$

2. $$
   f(kx)=kf(x)
   $$

上記に当てはまるのが線形、それ以外は非線形

基本的にグラフに描画した時まっすぐな線なら線形、ぐにゃぐにゃなら非線形



- 中間層用と出力層用の活性化関数が存在する
  - 中間層
    - ReLU関数：0(x<0),1(0<=x))
      - 最初の選択肢になることが多い
      - 勾配消失問題の回避とスパース化に貢献
      - 閾値を設けて値を超えたらそのまあ、超えなかったら0を渡す
    - シグモイド関数：ロジスティック
      - 0〜1の間を取るので強弱を伝えられるようになった
      - 勾配消失問題が起こる
    - ステップ関数⇨現在使われていない
      - ONとOFFしか学習できず、線形分離可能なものしか学習できない
  - 出力層
    - ソフトマックス関数
    - 恒等写像
    - シグモイド関数

z = f(u)のソースコード抜き出し

```python
z = functions.sigmoid(u)
```

## section3:出力層

- 誤差関数:二乗誤差と交差エントロピー

$$
\frac{1}{2}\sum_{j=1}^{I}(y_j-d_j)^2=\frac{I}{2}||(y-d)||^2\\
-\sum_{i=1}^{I}d_j\log y_i
$$

- この誤差関数を小さくすることが深層学習の目的になる

なぜ引き算ではなく二乗するのか。

```
誤差を明確にするため二乗してどれだけ離れているかを全てプラスの値にすることができる
```

上記数式の1/2とは

```
微分した時に計算を簡単にするため
```

- 分類問題の場合出力層の出力は0〜1の範囲に限定する必要がある

出力層

- ソフトマックス関数
  - 多クラス分類に使用される
  - 誤差関数は交差エントロピーを使用
- 恒等写像
  - 回帰に使用
  - 誤差関数は二乗誤差
- シグモイド関数
  - 二値分類に使用
  - 誤差関数は交差エントロピー

ソフトマックス関数について

```
1.def softmax(x):

2.np.exp(x)
eのx乗
3.np.sum(np.exp(x)
eのx乗の合計
```

交差エントロピー

```
1.def cross_entropy_error(d, y):

2.-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7))
```

## section4:勾配降下法

- 勾配降下法
  - 誤差E (w)を最小化するパラメータwを発見する
  - 学習率を大きくしすぎると最小値にいつまでもたどり着かない
  - 学習率を小さくすると収束するまで時間かかる
- 確率的勾配降下法
  - データが冗長の場、計算コストが減る⇨ランダムに抽出したサンプルの誤差を使用するため
  - オンライン学習可能
    - リアルタイムなデータを使用して、誤差逆伝播により即時パラメータの更新が可能
- ミニバッチ勾配降下法
  - ランダムに分割したデータの集合に属するサンプルの平均誤差
  - 確率的勾配降下法とは全体からサンプルとってるか、分割したものからとってるかの違い
  - 計算機の計算資源を有効活用できる（確率的勾配降下法のメリットを損なわず）
- 誤差勾配の計算
  - 数値微分：各パラメータそれぞれについて計算しなければいけない
  - 誤差逆伝播を利用する
- 深層学習の開発環境
  - ローカルの開発環境をクラウドサービスを用いてクラウドで行う
  - クラウドのコストパフォーマンス高い

## section5:誤差逆伝播法

- 算出された誤差を出力層側から順に微分して前の層へと伝播させる。
- 誤差から微分して逆算することで、不要な再帰的計算を避けて微分を算出できる

```python
def backward(x, d, z1, y):
    print("\n##### 誤差逆伝播開始 #####")

    grad = {}

    W1, W2 = network['W1'], network['W2']
    b1, b2 = network['b1'], network['b2']
    #  出力層でのデルタ
    delta2 = functions.d_sigmoid_with_loss(d, y)
    #  b2の勾配
    grad['b2'] = np.sum(delta2, axis=0)
    #  W2の勾配
    grad['W2'] = np.dot(z1.T, delta2)
    #  中間層でのデルタ
    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)
    # b1の勾配
    grad['b1'] = np.sum(delta1, axis=0)
    #  W1の勾配
    grad['W1'] = np.dot(x.T, delta1)
        
    print_vec("偏微分_dE/du2", delta2)
    print_vec("偏微分_dE/du2", delta1)

    print_vec("偏微分_重み1", grad["W1"])
    print_vec("偏微分_重み2", grad["W2"])
    print_vec("偏微分_バイアス1", grad["b1"])
    print_vec("偏微分_バイアス2", grad["b2"])

    return grad
    
# 訓練データ
x = np.array([[1.0, 5.0]])
# 目標出力
d = np.array([[0, 1]])
#  学習率
learning_rate = 0.01
network =  init_network()
y, z1 = forward(network, x)

# 誤差
loss = functions.cross_entropy_error(d, y)

grad = backward(x, d, z1, y)
for key in ('W1', 'W2', 'b1', 'b2'):
    network[key]  -= learning_rate * grad[key]

print("##### 結果表示 #####")    


print("##### 更新後パラメータ #####") 
print_vec("重み1", network['W1'])
print_vec("重み2", network['W2'])
print_vec("バイアス1", network['b1'])
print_vec("バイアス2", network['b2'])
```



# 深層学習　前半2

## Section1:勾配消失問題について

- 誤差逆伝播が改装に進んでいくにつれ勾配が緩やかになるため訓練が最適値に収束しなくなる
- 解決方法
  - 活性化関数の選択
    - ReLU関数：スパース化に貢献
  - 重みの初期値設定：0に設定すると全ての値が同じ値で伝わるためパラメータのチューニングができなくる
    - Xavier：重みの要素を、前の層のノード数の平方根で除算した値
    - He：重みの要素を、前の層のノード層の平方根で除算した値に対し√2をかけた値
  - バッチ正規化：ミニバッチ単位で入力値のデータの偏りを抑制する手法
    - 活性化関数に値を渡す前後に、バッチ正規化の処理を孕んだ層を加える
    - 深層学習の学習スピードを強化
    - 勾配消失が起きづらくなる

## Section2:学習率最適化

- 学習率最適化手法
  - モメンタム：誤差をパラメータで微分したもの値と学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の席を加算する
    - 局所的最適解にならず、大域的最適解となる
    - 最も低い位置に行くまでの時間が早い
  - AdaGrad：誤差をパラメータで微分したものと再定義した学習率の積を減算する
    - 勾配の穏やかな斜面に対して、最適値に近づける
  - RMSProp：誤差をパラメータで微分したものと再定義した学習率の積を減算する
    - 大域的最適解となる
    - ハイパーパラメータが必要な場合が少ない
  - Adam
    - モメンタムとRMSPropを孕んだ最適化アルゴリズムである
    - 上記の手法のメリットを孕んでいる

## section3:過学習

- 正則化：ネットワークの自由度を制約すること
- L1正則化、L2正則化
  - 誤差関数にpノルムを加える
- ドロップアウト
- Weight decay(荷重減衰)
  - 過学習が起こりそうな重みの大きさ以下で重みをコントロールする
- ドロップアウト
  - ランダムにノードを削除するメリットとしてデータ量を変化せずに、異なるモデルを学習させていると解釈できる

## section4:CNN

- 入力層⇨畳み込み層⇨プーリング層⇨繰り返し⇨全結合層⇨出力層
- 畳み込み層
  - 三次元データをそのまま学習して次に伝えることができる
    - パディング：入力画像の周りに固定データを広げるように埋める
    - ストライド：畳み込む範囲の移動距離
    - チャンネル：入力値を分解した層のこと
- プーリング層
  - 対象領域のマックス値または平均値を取得

## section5:AlexNet

五層の畳み込み層およびプーリング層など、それに続く三層の全結合層から構成される

過学習を防ぐため、サイズ４０９６の全結合層の出力にドロップアウトを使用している



## 深層学習前編の演習ファイル

