# 深層学習　前半

## ニューラルネットワーク全体像

- 入力層に与えらた値それぞれに重みがかけられて最後にバイアスが足されて、中間層に渡される。
- 重み、バイアスに関してはそれぞれの入力層、中間層に対して与えられている
- そもそもの深層学習自体の目的は誤差を最小化するパラメータを発見すること
  - 重みとバイアスが上記パラメータに含まれる

- NNできること
  - 回帰：予測すること
    - 結果予想や予想ランキング
  - 分類：判別やクラス分類
    - 写真の判別
    - 文字認識（OCR）
- 深層学習の実用例
  - 自動売買
  - チャットボット
  - 翻訳etc

## section1:入力層から中間層

- 入力層の値それぞれに対して重みをかけてバイアスを足す、結果として出力が次の層：中間層に渡される。

![UNADJUSTEDNONRAW_thumb_bda](/Users/koiketomoki/Pictures/写真ライブラリ.photoslibrary/resources/proxies/derivatives/0b/00/bda/UNADJUSTEDNONRAW_thumb_bda.jpg)

![深層学習前編](/Users/koiketomoki/Pictures/写真ライブラリ.photoslibrary/Masters/2019/06/24/20190624-102119/深層学習前編.png)



## section2:活性化関数

- NNにおいて次の層への出力の大きさを決める非線形の関数
- 次の層に渡すに当たって値に強弱をつける



線形と非線形の違い

1. $$
   f(x+y)=f(x)+f(y)
   $$

2. $$
   f(kx)=kf(x)
   $$

上記に当てはまるのが線形、それ以外は非線形

基本的にグラフに描画した時まっすぐな線なら線形、ぐにゃぐにゃなら非線形



- 中間層用と出力層用の活性化関数が存在する
  - 中間層
    - ReLU関数：0(x<0),1(0<=x))
      - 最初の選択肢になることが多い
      - 勾配消失問題の回避とスパース化に貢献
      - 閾値を設けて値を超えたらそのまあ、超えなかったら0を渡す
    - シグモイド関数：ロジスティック
      - 0〜1の間を取るので強弱を伝えられるようになった
      - 勾配消失問題が起こる
    - ステップ関数⇨現在使われていない
      - ONとOFFしか学習できず、線形分離可能なものしか学習できない
  - 出力層
    - ソフトマックス関数
    - 恒等写像
    - シグモイド関数

z = f(u)のソースコード抜き出し

```python
z = functions.sigmoid(u)
```



