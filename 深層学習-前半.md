# 深層学習　前半

## ニューラルネットワーク全体像

- 入力層に与えらた値それぞれに重みがかけられて最後にバイアスが足されて、中間層に渡される。
- 重み、バイアスに関してはそれぞれの入力層、中間層に対して与えられている
- そもそもの深層学習自体の目的は誤差を最小化するパラメータを発見すること
  - 重みとバイアスが上記パラメータに含まれる

- NNできること
  - 回帰：予測すること
    - 結果予想や予想ランキング
  - 分類：判別やクラス分類
    - 写真の判別
    - 文字認識（OCR）
- 深層学習の実用例
  - 自動売買
  - チャットボット
  - 翻訳etc

## section1:入力層から中間層

- 入力層の値それぞれに対して重みをかけてバイアスを足す、結果として出力が次の層：中間層に渡される。

![UNADJUSTEDNONRAW_thumb_bda](/Users/koiketomoki/Pictures/写真ライブラリ.photoslibrary/resources/proxies/derivatives/0b/00/bda/UNADJUSTEDNONRAW_thumb_bda.jpg)

![深層学習前編](/Users/koiketomoki/Pictures/写真ライブラリ.photoslibrary/Masters/2019/06/24/20190624-102119/深層学習前編.png)



## section2:活性化関数

- NNにおいて次の層への出力の大きさを決める非線形の関数
- 次の層に渡すに当たって値に強弱をつける



線形と非線形の違い

1. $$
   f(x+y)=f(x)+f(y)
   $$

2. $$
   f(kx)=kf(x)
   $$

上記に当てはまるのが線形、それ以外は非線形

基本的にグラフに描画した時まっすぐな線なら線形、ぐにゃぐにゃなら非線形



- 中間層用と出力層用の活性化関数が存在する
  - 中間層
    - ReLU関数：0(x<0),1(0<=x))
      - 最初の選択肢になることが多い
      - 勾配消失問題の回避とスパース化に貢献
      - 閾値を設けて値を超えたらそのまあ、超えなかったら0を渡す
    - シグモイド関数：ロジスティック
      - 0〜1の間を取るので強弱を伝えられるようになった
      - 勾配消失問題が起こる
    - ステップ関数⇨現在使われていない
      - ONとOFFしか学習できず、線形分離可能なものしか学習できない
  - 出力層
    - ソフトマックス関数
    - 恒等写像
    - シグモイド関数

z = f(u)のソースコード抜き出し

```python
z = functions.sigmoid(u)
```

## section3:出力層

- 誤差関数:二乗誤差と交差エントロピー

$$
\frac{1}{2}\sum_{j=1}^{I}(y_j-d_j)^2=\frac{I}{2}||(y-d)||^2\\
-\sum_{i=1}^{I}d_j\log y_i
$$

- この誤差関数を小さくすることが深層学習の目的になる

なぜ引き算ではなく二乗するのか。

```
誤差を明確にするため二乗してどれだけ離れているかを全てプラスの値にすることができる
```

上記数式の1/2とは

```
微分した時に計算を簡単にするため
```

- 分類問題の場合出力層の出力は0〜1の範囲に限定する必要がある

出力層

- ソフトマックス関数
  - 多クラス分類に使用される
  - 誤差関数は交差エントロピーを使用
- 恒等写像
  - 回帰に使用
  - 誤差関数は二乗誤差
- シグモイド関数
  - 二値分類に使用
  - 誤差関数は交差エントロピー

ソフトマックス関数について

```
1.def softmax(x):

2.np.exp(x)
eのx乗
3.np.sum(np.exp(x)
eのx乗の合計
```

交差エントロピー

```
1.def cross_entropy_error(d, y):

2.-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7))
```

## section4:勾配降下法

- 勾配降下法
  - 誤差E (w)を最小化するパラメータwを発見する
  - 学習率を大きくしすぎると最小値にいつまでもたどり着かない
  - 学習率を小さくすると収束するまで時間かかる
- 確率的勾配降下法
  - データが冗長の場、計算コストが減る⇨ランダムに抽出したサンプルの誤差を使用するため
  - オンライン学習可能
    - リアルタイムなデータを使用して、誤差逆伝播により即時パラメータの更新が可能
- ミニバッチ勾配降下法
  - ランダムに分割したデータの集合に属するサンプルの平均誤差
  - 確率的勾配降下法とは全体からサンプルとってるか、分割したものからとってるかの違い
  - 計算機の計算資源を有効活用できる（確率的勾配降下法のメリットを損なわず）
- 誤差勾配の計算
  - 数値微分：各パラメータそれぞれについて計算しなければいけない
  - 誤差逆伝播を利用する
- 深層学習の開発環境
  - ローカルの開発環境をクラウドサービスを用いてクラウドで行う
  - クラウドのコストパフォーマンス高い

## section5:誤差逆伝播法

- 算出された誤差を出力層側から順に微分して前の層へと伝播させる。
- 誤差から微分して逆算することで、不要な再帰的計算を避けて微分を算出できる

```python
def backward(x, d, z1, y):
    print("\n##### 誤差逆伝播開始 #####")

    grad = {}

    W1, W2 = network['W1'], network['W2']
    b1, b2 = network['b1'], network['b2']
    #  出力層でのデルタ
    delta2 = functions.d_sigmoid_with_loss(d, y)
    #  b2の勾配
    grad['b2'] = np.sum(delta2, axis=0)
    #  W2の勾配
    grad['W2'] = np.dot(z1.T, delta2)
    #  中間層でのデルタ
    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)
    # b1の勾配
    grad['b1'] = np.sum(delta1, axis=0)
    #  W1の勾配
    grad['W1'] = np.dot(x.T, delta1)
        
    print_vec("偏微分_dE/du2", delta2)
    print_vec("偏微分_dE/du2", delta1)

    print_vec("偏微分_重み1", grad["W1"])
    print_vec("偏微分_重み2", grad["W2"])
    print_vec("偏微分_バイアス1", grad["b1"])
    print_vec("偏微分_バイアス2", grad["b2"])

    return grad
    
# 訓練データ
x = np.array([[1.0, 5.0]])
# 目標出力
d = np.array([[0, 1]])
#  学習率
learning_rate = 0.01
network =  init_network()
y, z1 = forward(network, x)

# 誤差
loss = functions.cross_entropy_error(d, y)

grad = backward(x, d, z1, y)
for key in ('W1', 'W2', 'b1', 'b2'):
    network[key]  -= learning_rate * grad[key]

print("##### 結果表示 #####")    


print("##### 更新後パラメータ #####") 
print_vec("重み1", network['W1'])
print_vec("重み2", network['W2'])
print_vec("バイアス1", network['b1'])
print_vec("バイアス2", network['b2'])
```

Jupiter演習等

